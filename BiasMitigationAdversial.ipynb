{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81d0c17-88c4-47fd-9152-fcb6e89040ed",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945c466d-731f-42b5-aa4f-3cc0685c5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/_9/51_3yw1x3db244x7mjkk66h80000gn/T/pip-req-build-wakqic1m\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/_9/51_3yw1x3db244x7mjkk66h80000gn/T/pip-req-build-wakqic1m\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies ... \u001b[?done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (6.3.1)\n",
      "Requirement already satisfied: regex in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: torchvision in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: wcwidth in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: packaging in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: torch in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from clip==1.0) (2.6.0)\n",
      "Requirement already satisfied: numpy in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from torch->clip==1.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from torch->clip==1.0) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/vishwasparekh/Desktop/University of Southern California/CSCI-544/Assignments/HW2/myenv/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ftfy regex tqdm git+https://github.com/openai/CLIP.git torchvision\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import clip\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447b71d-20b2-4f02-8cc4-b1e935031542",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8180496-c788-41ee-af98-3125d9dabeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAwareAdversary(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_size=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.debias_head = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.debias_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63beb9b-7f6b-4ed4-9deb-7e97e3023c10",
   "metadata": {},
   "source": [
    "## Defining helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9e081f-4f22-4913-84ac-0a77ebd3c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def create_debias_dataset(data_path):\n",
    "    return ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=preprocess\n",
    "    )\n",
    "\n",
    "def entropy_loss(predictions):\n",
    "    softmax = torch.nn.functional.softmax(predictions, dim=-1)\n",
    "    log_softmax = torch.nn.functional.log_softmax(predictions, dim=-1)\n",
    "    return torch.sum(softmax * log_softmax, dim=-1).mean()\n",
    "\n",
    "def gradient_penalty(image_features, adversary_preds):\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=adversary_preds.sum(),\n",
    "        inputs=image_features,\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    return torch.mean(gradients.pow(2).sum(dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fd8da-2a47-4c1b-9eb4-309cb04bd8f2",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1651c64b-c1a5-49fc-8890-e192c81b32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint_debias(clip_model, adversary_model, train_loader, num_epochs=10):\n",
    "    # Optimizers with differential learning rates\n",
    "    clip_optim = torch.optim.AdamW(\n",
    "        clip_model.parameters(), \n",
    "        lr=1e-6,  # Small LR for CLIP\n",
    "        weight_decay=0.05\n",
    "    )\n",
    "    adv_optim = torch.optim.AdamW(\n",
    "        adversary_model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    classification_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward through CLIP (trainable)\n",
    "            image_features = clip_model.encode_image(images)\n",
    "            \n",
    "            # Adversarial predictions\n",
    "            adv_preds = adversary_model(image_features)\n",
    "            \n",
    "            # Loss components\n",
    "            task_loss = classification_loss(adv_preds, labels)\n",
    "            gp_loss = gradient_penalty(image_features, adv_preds)\n",
    "            \n",
    "            # Static feature debiasing\n",
    "            with torch.no_grad():\n",
    "                static_features = image_features.mean(dim=0, keepdim=True).repeat(images.size(0), 1)\n",
    "            debias_loss = -entropy_loss(adversary_model(static_features))  # Maximize entropy\n",
    "            \n",
    "            # Combined loss (λ weights from paper)\n",
    "            loss = task_loss + 0.7*debias_loss + 0.2*gp_loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            clip_optim.zero_grad()\n",
    "            adv_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for CLIP stability\n",
    "            torch.nn.utils.clip_grad_norm_(clip_model.parameters(), 1.0)\n",
    "            \n",
    "            clip_optim.step()\n",
    "            adv_optim.step()\n",
    "            \n",
    "            # Update progress\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92522b6-1d77-4d0c-8366-bd15e38efbe6",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4272a76-b093-4959-9271-941b0e3462ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed764a7dc511401da2c752f5bc08c0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 2.4710\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec14ec3b422943378833d0db390332c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 1.9116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993eb3c44fb24e98974cae99d6941092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 1.3759\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73854289852e458fab4b9cc03fe9a79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 0.9172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99eb0cd226654d47a5979440cd8b533f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Avg Loss: 0.6743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43d8c425c314b0dac4a3195d91f2dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Avg Loss: 0.5172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f8a1c032c64885afd767a16d5ad050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Avg Loss: 0.4014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d13ee7fbf844739adeb8468e55e37c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Avg Loss: 0.3251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4cca80ab2a49d7a420e706eb7a08b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Avg Loss: 0.2613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d180e3460f4465bd3cfae58316fcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Avg Loss: 0.2292\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Dataset\"\n",
    "batch_size = 64\n",
    "\n",
    "# Create datasets and loader\n",
    "train_dataset = create_debias_dataset(DATA_PATH)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize models\n",
    "adversary_model = BiasAwareAdversary(\n",
    "    num_classes=len(train_dataset.classes)\n",
    ").to(device)\n",
    "\n",
    "# Ensure CLIP is trainable\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Start joint training\n",
    "train_joint_debias(\n",
    "    clip_model=clip_model,\n",
    "    adversary_model=adversary_model,\n",
    "    train_loader=train_loader,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9cfe4-5e47-4e2c-a6ae-cc0dd4793bb5",
   "metadata": {},
   "source": [
    "## Loading necessary data and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a495cc-1ae5-4b29-84d4-a61fe0ea9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"wordlists\", exist_ok=True)\n",
    "urllib.request.urlretrieve(\n",
    "    \"http://ptrckprry.com/course/ssd/data/positive-words.txt\",\n",
    "    \"wordlists/positive-words.txt\"\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    \"http://ptrckprry.com/course/ssd/data/negative-words.txt\",\n",
    "    \"wordlists/negative-words.txt\"\n",
    ")\n",
    "\n",
    "def load_images(folder, limit=None):\n",
    "    paths = sorted(glob(os.path.join(folder, \"*.jpg\")))[:limit]\n",
    "    tensors = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            image = preprocess(Image.open(p).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            tensors.append(image)\n",
    "        except:\n",
    "            continue\n",
    "    if tensors:\n",
    "        return torch.cat(tensors)\n",
    "    return None\n",
    "\n",
    "def embed_images(folder, limit=None):\n",
    "    imgs = load_images(folder, limit)\n",
    "    if imgs is None: return None\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.encode_image(imgs).float()\n",
    "    return emb\n",
    "\n",
    "def collect_all_embeddings(root_path, limit=None):\n",
    "    target_groups = [\n",
    "        \"Religion\", \n",
    "        \"Nationality\", \n",
    "        \"Disability\", \n",
    "        \"Sexual Orientation\", \n",
    "        \"Valence Images\"\n",
    "    ]\n",
    "    \n",
    "    embeddings = {}\n",
    "    for category in target_groups:\n",
    "        cat_path = os.path.join(root_path, category)\n",
    "        if not os.path.isdir(cat_path):\n",
    "            continue\n",
    "        embeddings[category] = {}\n",
    "        for subgroup in os.listdir(cat_path):\n",
    "            sub_path = os.path.join(cat_path, subgroup)\n",
    "            if not os.path.isdir(sub_path):\n",
    "                continue\n",
    "            print(f\"Embedding {category}/{subgroup}\")\n",
    "            emb = embed_images(sub_path, limit)\n",
    "            if emb is not None:\n",
    "                embeddings[category][subgroup] = emb\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ff339-516b-4d5a-8661-64133477d774",
   "metadata": {},
   "source": [
    "## Loading words, embedding them and creating sentiment lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e679cc84-60f8-432c-b1d8-3bdf3bcf0fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1904 positive and 4658 negative words...\n",
      "✅ Text embedding complete.\n"
     ]
    }
   ],
   "source": [
    "def load_words(filepath, prefix=\"This is\"):\n",
    "    with open(filepath, encoding='latin1') as f:\n",
    "        lines = [w.strip() for w in f if w.strip() and not w.startswith(\";\")]\n",
    "    clean = [line for line in lines if line.isascii() and line.isalpha()]\n",
    "    return [f\"{prefix} {w}.\" for w in clean]\n",
    "\n",
    "def batch_tokenize_and_embed(text_list, batch_size=64):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "        tokens = clip.tokenize(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = clip_model.encode_text(tokens).float()\n",
    "        all_embeddings.append(emb.cpu())\n",
    "    return torch.cat(all_embeddings)\n",
    "\n",
    "# Load positive and negative words\n",
    "pos_words = load_words(\"wordlists/positive-words.txt\")\n",
    "neg_words = load_words(\"wordlists/negative-words.txt\")\n",
    "all_words = pos_words + neg_words\n",
    "\n",
    "sentiment_lookup = {}\n",
    "for word in pos_words:\n",
    "    sentiment_lookup[word] = \"positive\"\n",
    "for word in neg_words:\n",
    "    sentiment_lookup[word] = \"negative\"\n",
    "\n",
    "print(f\"Embedding {len(pos_words)} positive and {len(neg_words)} negative words...\")\n",
    "pos_emb = batch_tokenize_and_embed(pos_words)\n",
    "neg_emb = batch_tokenize_and_embed(neg_words)\n",
    "all_word_emb = torch.cat([pos_emb, neg_emb])\n",
    "print(\"✅ Text embedding complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280f77b-84c4-421f-8325-daba5bd5018c",
   "metadata": {},
   "source": [
    "## Functions for loading top attributes and computing pairwise bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5856c9e-1691-4f68-87ae-54784cc6d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_attributes(emb, all_words, all_word_emb, top_k=15):\n",
    "    avg_emb = emb.mean(dim=0, keepdim=True)\n",
    "    sims = cosine_similarity(avg_emb.cpu().numpy(), all_word_emb.cpu().numpy())[0]\n",
    "    indices = np.argsort(sims)[::-1][:top_k]\n",
    "    return [(all_words[i], sims[i]) for i in indices]\n",
    "\n",
    "def caliskan_score(X, Y, A, B):\n",
    "    def s(w): return cosine_similarity(w.cpu(), A.cpu()).mean() - cosine_similarity(w.cpu(), B.cpu()).mean()\n",
    "    s_X = torch.tensor([s(x.unsqueeze(0)) for x in X])\n",
    "    s_Y = torch.tensor([s(y.unsqueeze(0)) for y in Y])\n",
    "    return ((s_X.mean() - s_Y.mean()) / torch.std(torch.cat([s_X, s_Y]))).item()\n",
    "\n",
    "def compute_pairwise_bias(image_embeddings, A, B, group_label):\n",
    "    rows = []\n",
    "    subgroups = list(image_embeddings.keys())\n",
    "\n",
    "    for g1, g2 in itertools.combinations(subgroups, 2):\n",
    "        emb1 = image_embeddings[g1]\n",
    "        emb2 = image_embeddings[g2]\n",
    "        score = caliskan_score(emb1, emb2, A, B)\n",
    "        favored = g1 if score > 0 else g2 if score < 0 else \"neutral\"\n",
    "        rows.append({\n",
    "            \"Group\": group_label,\n",
    "            \"Subgroup_A\": g1,\n",
    "            \"Subgroup_B\": g2,\n",
    "            \"Bias_Score\": score,\n",
    "            \"Favored_Group\": favored\n",
    "        })\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c20e49-7cba-49c0-b52a-3fd5e0b13cab",
   "metadata": {},
   "source": [
    "## Evaluating bias and storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a0f910f-7993-4f83-8251-89bf7835b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Religion/Christians\n",
      "Embedding Religion/Muslims\n",
      "Embedding Religion/Hindu\n",
      "Embedding Religion/Jewish\n",
      "Embedding Religion/Sikhs\n",
      "Embedding Religion/Buddists\n",
      "Embedding Nationality/Indian\n",
      "Embedding Nationality/European\n",
      "Embedding Nationality/Americans\n",
      "Embedding Nationality/Chinese\n",
      "Embedding Nationality/Arab\n",
      "Embedding Nationality/Mexican\n",
      "Embedding Disability/Non-Disabled\n",
      "Embedding Disability/Mentally Disabled\n",
      "Embedding Disability/Physically Disabled\n",
      "Embedding Sexual Orientation/Lesbian\n",
      "Embedding Sexual Orientation/Gay\n",
      "Embedding Sexual Orientation/Heterosexual\n",
      "Embedding Sexual Orientation/Transgender\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Subgroup_A</th>\n",
       "      <th>Subgroup_B</th>\n",
       "      <th>Bias_Score</th>\n",
       "      <th>Favored_Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Disability</td>\n",
       "      <td>Mentally Disabled</td>\n",
       "      <td>Physically Disabled</td>\n",
       "      <td>-1.377831</td>\n",
       "      <td>Physically Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Disability</td>\n",
       "      <td>Non-Disabled</td>\n",
       "      <td>Physically Disabled</td>\n",
       "      <td>-0.319904</td>\n",
       "      <td>Physically Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Disability</td>\n",
       "      <td>Non-Disabled</td>\n",
       "      <td>Mentally Disabled</td>\n",
       "      <td>1.166855</td>\n",
       "      <td>Non-Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>0.429248</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Indian</td>\n",
       "      <td>European</td>\n",
       "      <td>0.847663</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Arab</td>\n",
       "      <td>-0.057382</td>\n",
       "      <td>Arab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>European</td>\n",
       "      <td>Americans</td>\n",
       "      <td>-0.299404</td>\n",
       "      <td>Americans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>European</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>-0.295654</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>European</td>\n",
       "      <td>Arab</td>\n",
       "      <td>-0.927816</td>\n",
       "      <td>Arab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>European</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>-0.492094</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Americans</td>\n",
       "      <td>Arab</td>\n",
       "      <td>-0.630326</td>\n",
       "      <td>Arab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Americans</td>\n",
       "      <td>0.557671</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Americans</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>-0.168576</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Arab</td>\n",
       "      <td>-0.660651</td>\n",
       "      <td>Arab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>-0.188796</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Arab</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>0.503435</td>\n",
       "      <td>Arab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Americans</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.014211</td>\n",
       "      <td>Americans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Nationality</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.585709</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Christians</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>-0.397222</td>\n",
       "      <td>Muslims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>-0.621803</td>\n",
       "      <td>Hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Buddists</td>\n",
       "      <td>-0.738555</td>\n",
       "      <td>Buddists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>Sikhs</td>\n",
       "      <td>-0.926662</td>\n",
       "      <td>Sikhs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>Buddists</td>\n",
       "      <td>0.976903</td>\n",
       "      <td>Hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>Sikhs</td>\n",
       "      <td>0.341819</td>\n",
       "      <td>Hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>1.291335</td>\n",
       "      <td>Hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>Buddists</td>\n",
       "      <td>0.267255</td>\n",
       "      <td>Muslims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>Sikhs</td>\n",
       "      <td>-0.196319</td>\n",
       "      <td>Sikhs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>0.854598</td>\n",
       "      <td>Muslims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Sikhs</td>\n",
       "      <td>Buddists</td>\n",
       "      <td>0.447001</td>\n",
       "      <td>Sikhs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Christians</td>\n",
       "      <td>Buddists</td>\n",
       "      <td>-0.200276</td>\n",
       "      <td>Buddists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Christians</td>\n",
       "      <td>Sikhs</td>\n",
       "      <td>-0.532627</td>\n",
       "      <td>Sikhs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Christians</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>0.486029</td>\n",
       "      <td>Christians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Religion</td>\n",
       "      <td>Christians</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>-0.933951</td>\n",
       "      <td>Hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sexual Orientation</td>\n",
       "      <td>Gay</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>-0.057226</td>\n",
       "      <td>Heterosexual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sexual Orientation</td>\n",
       "      <td>Lesbian</td>\n",
       "      <td>Transgender</td>\n",
       "      <td>0.048678</td>\n",
       "      <td>Lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sexual Orientation</td>\n",
       "      <td>Gay</td>\n",
       "      <td>Transgender</td>\n",
       "      <td>-0.185764</td>\n",
       "      <td>Transgender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sexual Orientation</td>\n",
       "      <td>Lesbian</td>\n",
       "      <td>Gay</td>\n",
       "      <td>0.237486</td>\n",
       "      <td>Lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sexual Orientation</td>\n",
       "      <td>Lesbian</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>0.179478</td>\n",
       "      <td>Lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sexual Orientation</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>Transgender</td>\n",
       "      <td>-0.127496</td>\n",
       "      <td>Transgender</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Group         Subgroup_A           Subgroup_B  Bias_Score  \\\n",
       "32          Disability  Mentally Disabled  Physically Disabled   -1.377831   \n",
       "31          Disability       Non-Disabled  Physically Disabled   -0.319904   \n",
       "30          Disability       Non-Disabled    Mentally Disabled    1.166855   \n",
       "19         Nationality             Indian              Mexican    0.429248   \n",
       "15         Nationality             Indian             European    0.847663   \n",
       "18         Nationality             Indian                 Arab   -0.057382   \n",
       "20         Nationality           European            Americans   -0.299404   \n",
       "21         Nationality           European              Chinese   -0.295654   \n",
       "22         Nationality           European                 Arab   -0.927816   \n",
       "23         Nationality           European              Mexican   -0.492094   \n",
       "25         Nationality          Americans                 Arab   -0.630326   \n",
       "16         Nationality             Indian            Americans    0.557671   \n",
       "26         Nationality          Americans              Mexican   -0.168576   \n",
       "27         Nationality            Chinese                 Arab   -0.660651   \n",
       "28         Nationality            Chinese              Mexican   -0.188796   \n",
       "29         Nationality               Arab              Mexican    0.503435   \n",
       "24         Nationality          Americans              Chinese    0.014211   \n",
       "17         Nationality             Indian              Chinese    0.585709   \n",
       "0             Religion         Christians              Muslims   -0.397222   \n",
       "5             Religion            Muslims                Hindu   -0.621803   \n",
       "13            Religion             Jewish             Buddists   -0.738555   \n",
       "12            Religion             Jewish                Sikhs   -0.926662   \n",
       "11            Religion              Hindu             Buddists    0.976903   \n",
       "10            Religion              Hindu                Sikhs    0.341819   \n",
       "9             Religion              Hindu               Jewish    1.291335   \n",
       "8             Religion            Muslims             Buddists    0.267255   \n",
       "7             Religion            Muslims                Sikhs   -0.196319   \n",
       "6             Religion            Muslims               Jewish    0.854598   \n",
       "14            Religion              Sikhs             Buddists    0.447001   \n",
       "4             Religion         Christians             Buddists   -0.200276   \n",
       "3             Religion         Christians                Sikhs   -0.532627   \n",
       "2             Religion         Christians               Jewish    0.486029   \n",
       "1             Religion         Christians                Hindu   -0.933951   \n",
       "36  Sexual Orientation                Gay         Heterosexual   -0.057226   \n",
       "35  Sexual Orientation            Lesbian          Transgender    0.048678   \n",
       "37  Sexual Orientation                Gay          Transgender   -0.185764   \n",
       "33  Sexual Orientation            Lesbian                  Gay    0.237486   \n",
       "34  Sexual Orientation            Lesbian         Heterosexual    0.179478   \n",
       "38  Sexual Orientation       Heterosexual          Transgender   -0.127496   \n",
       "\n",
       "          Favored_Group  \n",
       "32  Physically Disabled  \n",
       "31  Physically Disabled  \n",
       "30         Non-Disabled  \n",
       "19               Indian  \n",
       "15               Indian  \n",
       "18                 Arab  \n",
       "20            Americans  \n",
       "21              Chinese  \n",
       "22                 Arab  \n",
       "23              Mexican  \n",
       "25                 Arab  \n",
       "16               Indian  \n",
       "26              Mexican  \n",
       "27                 Arab  \n",
       "28              Mexican  \n",
       "29                 Arab  \n",
       "24            Americans  \n",
       "17               Indian  \n",
       "0               Muslims  \n",
       "5                 Hindu  \n",
       "13             Buddists  \n",
       "12                Sikhs  \n",
       "11                Hindu  \n",
       "10                Hindu  \n",
       "9                 Hindu  \n",
       "8               Muslims  \n",
       "7                 Sikhs  \n",
       "6               Muslims  \n",
       "14                Sikhs  \n",
       "4              Buddists  \n",
       "3                 Sikhs  \n",
       "2            Christians  \n",
       "1                 Hindu  \n",
       "36         Heterosexual  \n",
       "35              Lesbian  \n",
       "37          Transgender  \n",
       "33              Lesbian  \n",
       "34              Lesbian  \n",
       "38          Transgender  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_attributes_bias_rows = []\n",
    "\n",
    "embeddings = collect_all_embeddings(\"Dataset\")\n",
    "\n",
    "for group_name in [\"Religion\", \"Nationality\", \"Disability\", \"Sexual Orientation\"]:\n",
    "    subgroup_embeddings = embeddings[group_name]\n",
    "    \n",
    "    rows_word_attributes = compute_pairwise_bias(subgroup_embeddings, pos_emb, neg_emb, group_name)\n",
    "    word_attributes_bias_rows.extend(rows_word_attributes)\n",
    "\n",
    "df_word_attributes = pd.DataFrame(word_attributes_bias_rows)\n",
    "display(df_word_attributes.sort_values(by=\"Group\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
